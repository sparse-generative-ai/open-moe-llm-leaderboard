[TensorRT-LLM] TensorRT-LLM version: 0.9.0
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
0.9.0
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:18,  1.04s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:02<00:26,  1.56s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:04<00:27,  1.74s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:06<00:27,  1.82s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:08<00:26,  1.88s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:10<00:24,  1.89s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:11<00:18,  1.55s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:13<00:17,  1.63s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:15<00:17,  1.72s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:17<00:15,  1.76s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:19<00:14,  1.79s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:19<00:10,  1.52s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:21<00:09,  1.60s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:23<00:08,  1.67s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:25<00:06,  1.71s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:27<00:05,  1.75s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:28<00:02,  1.47s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:29<00:01,  1.57s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:31<00:00,  1.61s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:31<00:00,  1.66s/it]
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Weights loaded. Total time: 00:00:21
Weights loaded. Total time: 00:00:23
Weights loaded. Total time: 00:00:30
Weights loaded. Total time: 00:00:30
Total time of converting checkpoints: 00:03:35
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
0.9.0
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:30,  1.68s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:32,  1.88s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:31,  1.96s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:29,  1.99s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:28,  2.04s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:26,  2.05s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:14<00:24,  2.05s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:16<00:22,  2.04s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:20,  2.01s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.96s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:21<00:15,  1.94s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:22<00:11,  1.63s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:24<00:10,  1.67s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:08,  1.73s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:28<00:07,  1.76s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:29<00:05,  1.78s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:30<00:02,  1.50s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:32<00:01,  1.58s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:34<00:00,  1.62s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:34<00:00,  1.80s/it]
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Weights loaded. Total time: 00:01:49
Weights loaded. Total time: 00:01:45
Weights loaded. Total time: 00:01:44
Weights loaded. Total time: 00:01:49
Total time of converting checkpoints: 00:08:38
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
0.9.0
Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]Loading checkpoint shards:   5%|▌         | 1/19 [00:01<00:24,  1.36s/it]Loading checkpoint shards:  11%|█         | 2/19 [00:03<00:30,  1.81s/it]Loading checkpoint shards:  16%|█▌        | 3/19 [00:05<00:31,  1.97s/it]Loading checkpoint shards:  21%|██        | 4/19 [00:07<00:30,  2.01s/it]Loading checkpoint shards:  26%|██▋       | 5/19 [00:09<00:28,  2.01s/it]Loading checkpoint shards:  32%|███▏      | 6/19 [00:11<00:25,  1.99s/it]Loading checkpoint shards:  37%|███▋      | 7/19 [00:13<00:23,  1.99s/it]Loading checkpoint shards:  42%|████▏     | 8/19 [00:15<00:21,  1.99s/it]Loading checkpoint shards:  47%|████▋     | 9/19 [00:17<00:19,  1.99s/it]Loading checkpoint shards:  53%|█████▎    | 10/19 [00:19<00:17,  1.96s/it]Loading checkpoint shards:  58%|█████▊    | 11/19 [00:21<00:15,  1.95s/it]Loading checkpoint shards:  63%|██████▎   | 12/19 [00:23<00:13,  1.94s/it]Loading checkpoint shards:  68%|██████▊   | 13/19 [00:25<00:11,  1.96s/it]Loading checkpoint shards:  74%|███████▎  | 14/19 [00:26<00:08,  1.64s/it]Loading checkpoint shards:  79%|███████▉  | 15/19 [00:27<00:06,  1.66s/it]Loading checkpoint shards:  84%|████████▍ | 16/19 [00:29<00:05,  1.73s/it]Loading checkpoint shards:  89%|████████▉ | 17/19 [00:31<00:03,  1.75s/it]Loading checkpoint shards:  95%|█████████▍| 18/19 [00:32<00:01,  1.49s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:34<00:00,  1.48s/it]Loading checkpoint shards: 100%|██████████| 19/19 [00:34<00:00,  1.79s/it]
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Weights loaded. Total time: 00:03:03
Weights loaded. Total time: 00:03:02
Weights loaded. Total time: 00:03:04
Weights loaded. Total time: 00:03:01
Total time of converting checkpoints: 00:13:15
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
[05/30/2024-11:59:30] [TRT-LLM] [I] Set bert_attention_plugin to float16.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set gpt_attention_plugin to float16.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set gemm_plugin to bfloat16.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set lookup_plugin to None.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set lora_plugin to None.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set moe_plugin to float16.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set context_fmha to True.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set paged_kv_cache to True.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set remove_input_padding to True.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set multi_block_mode to False.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set enable_xqa to True.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set tokens_per_block to 128.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set use_paged_context_fmha to False.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set use_fp8_context_fmha to False.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set multiple_profiles to False.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set paged_state to True.
[05/30/2024-11:59:30] [TRT-LLM] [I] Set streamingllm to False.
[05/30/2024-11:59:30] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. 
It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.
[05/30/2024-11:59:30] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. 

[05/30/2024-11:59:32] [TRT] [I] [MemUsageChange] Init CUDA: CPU +15, GPU +0, now: CPU 651, GPU 423 (MiB)
[05/30/2024-11:59:44] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1985, GPU +350, now: CPU 2772, GPU 773 (MiB)
[05/30/2024-11:59:44] [TRT-LLM] [I] Set nccl_plugin to bfloat16.
[05/30/2024-11:59:44] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-11:59:44] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0
[05/30/2024-11:59:44] [TRT] [W] Unused Input: position_ids
[05/30/2024-11:59:44] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.
[05/30/2024-11:59:45] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2812, GPU 799 (MiB)
[05/30/2024-11:59:45] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +2, GPU +10, now: CPU 2814, GPU 809 (MiB)
[05/30/2024-11:59:45] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.2
[05/30/2024-11:59:45] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.
[05/30/2024-11:59:45] [TRT] [E] 9: LLaMAForCausalLM/transformer/layers/0/attention/PLUGIN_V2_GPTAttention_0: could not find any supported formats consistent with input/output data types
[05/30/2024-11:59:45] [TRT] [E] 9: [pluginV2Builder.cpp::reportPluginError::24] Error Code 9: Internal Error (LLaMAForCausalLM/transformer/layers/0/attention/PLUGIN_V2_GPTAttention_0: could not find any supported formats consistent with input/output data types)
[05/30/2024-11:59:45] [TRT-LLM] [E] Engine building failed, please check the error log.
[05/30/2024-11:59:45] [TRT] [I] Serialized 59 bytes of code generator cache.
[05/30/2024-11:59:45] [TRT] [I] Serialized 0 timing cache entries
[05/30/2024-11:59:45] [TRT-LLM] [I] Timing cache serialized to model.cache
[05/30/2024-11:59:46] [TRT-LLM] [I] Serializing engine to mixtral-8x7b-v0.1-instruct-engine-bfloat16/rank0.engine...
Traceback (most recent call last):
  File "/usr/local/bin/trtllm-build", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 440, in main
    parallel_build(source, build_config, args.output_dir, workers,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 332, in parallel_build
    passed = build_and_save(rank, rank % workers, ckpt_dir,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 298, in build_and_save
    engine.save(output_dir)
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/builder.py", line 566, in save
    serialize_engine(
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/_common.py", line 105, in serialize_engine
    f.write(engine)
TypeError: a bytes-like object is required, not 'NoneType'
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
[05/30/2024-12:00:03] [TRT-LLM] [I] Set bert_attention_plugin to float16.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set gpt_attention_plugin to float16.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set gemm_plugin to bfloat16.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set lookup_plugin to None.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set lora_plugin to None.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set moe_plugin to float16.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set context_fmha to True.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set paged_kv_cache to True.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set remove_input_padding to True.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set multi_block_mode to False.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set enable_xqa to True.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set tokens_per_block to 128.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set use_paged_context_fmha to False.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set use_fp8_context_fmha to False.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set multiple_profiles to False.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set paged_state to True.
[05/30/2024-12:00:03] [TRT-LLM] [I] Set streamingllm to False.
[05/30/2024-12:00:03] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. 
It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.
[05/30/2024-12:00:03] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. 

[05/30/2024-12:00:05] [TRT] [I] [MemUsageChange] Init CUDA: CPU +15, GPU +0, now: CPU 651, GPU 423 (MiB)
[05/30/2024-12:00:09] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1985, GPU +350, now: CPU 2772, GPU 773 (MiB)
[05/30/2024-12:00:09] [TRT-LLM] [I] Set nccl_plugin to bfloat16.
[05/30/2024-12:00:09] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:09] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:10] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0
[05/30/2024-12:00:10] [TRT] [W] Unused Input: position_ids
[05/30/2024-12:00:10] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.
[05/30/2024-12:00:10] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2812, GPU 799 (MiB)
[05/30/2024-12:00:10] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +2, GPU +10, now: CPU 2814, GPU 809 (MiB)
[05/30/2024-12:00:10] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.2
[05/30/2024-12:00:10] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.
[05/30/2024-12:00:10] [TRT] [E] 9: LLaMAForCausalLM/transformer/layers/0/attention/PLUGIN_V2_GPTAttention_0: could not find any supported formats consistent with input/output data types
[05/30/2024-12:00:10] [TRT] [E] 9: [pluginV2Builder.cpp::reportPluginError::24] Error Code 9: Internal Error (LLaMAForCausalLM/transformer/layers/0/attention/PLUGIN_V2_GPTAttention_0: could not find any supported formats consistent with input/output data types)
[05/30/2024-12:00:10] [TRT-LLM] [E] Engine building failed, please check the error log.
[05/30/2024-12:00:10] [TRT] [I] Serialized 59 bytes of code generator cache.
[05/30/2024-12:00:10] [TRT] [I] Serialized 0 timing cache entries
[05/30/2024-12:00:10] [TRT-LLM] [I] Timing cache serialized to model.cache
[05/30/2024-12:00:10] [TRT-LLM] [I] Serializing engine to mixtral-8x7b-v0.1-instruct-engine-bfloat16/rank0.engine...
Traceback (most recent call last):
  File "/usr/local/bin/trtllm-build", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 440, in main
    parallel_build(source, build_config, args.output_dir, workers,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 332, in parallel_build
    passed = build_and_save(rank, rank % workers, ckpt_dir,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 298, in build_and_save
    engine.save(output_dir)
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/builder.py", line 566, in save
    serialize_engine(
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/_common.py", line 105, in serialize_engine
    f.write(engine)
TypeError: a bytes-like object is required, not 'NoneType'
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
[05/30/2024-12:00:15] [TRT-LLM] [I] Set bert_attention_plugin to float16.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set gpt_attention_plugin to float16.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set gemm_plugin to None.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set lookup_plugin to None.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set lora_plugin to None.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set moe_plugin to float16.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set context_fmha to True.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set paged_kv_cache to True.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set remove_input_padding to True.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set multi_block_mode to False.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set enable_xqa to True.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set tokens_per_block to 128.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set use_paged_context_fmha to False.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set use_fp8_context_fmha to False.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set multiple_profiles to False.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set paged_state to True.
[05/30/2024-12:00:15] [TRT-LLM] [I] Set streamingllm to False.
[05/30/2024-12:00:15] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. 
It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.
[05/30/2024-12:00:15] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. 

[05/30/2024-12:00:17] [TRT] [I] [MemUsageChange] Init CUDA: CPU +15, GPU +0, now: CPU 651, GPU 423 (MiB)
[05/30/2024-12:00:21] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1985, GPU +350, now: CPU 2772, GPU 773 (MiB)
[05/30/2024-12:00:21] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to bfloat16.
[05/30/2024-12:00:21] [TRT-LLM] [I] Set nccl_plugin to bfloat16.
[05/30/2024-12:00:21] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:21] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:22] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0
[05/30/2024-12:00:22] [TRT] [W] Unused Input: position_ids
[05/30/2024-12:00:22] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor input_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor position_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor last_token_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor kv_cache_block_pointers, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor host_kv_cache_block_pointers, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor sequence_length, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor host_request_types, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor host_past_key_value_lengths, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor context_lengths, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor host_context_lengths, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor host_max_attention_window_sizes, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor host_sink_token_length, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor cache_indirection, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/vocab_embedding/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/ELEMENTWISE_SUB_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/GATHER_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/SHUFFLE_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/SHUFFLE_9_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/PLUGIN_V2_AllGather_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/SLICE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/SLICE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/SLICE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/SLICE_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] Missing scale and zero-point for tensor logits, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:22] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.
[05/30/2024-12:00:22] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2814, GPU 799 (MiB)
[05/30/2024-12:00:22] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 2815, GPU 809 (MiB)
[05/30/2024-12:00:22] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.2
[05/30/2024-12:00:22] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.
[05/30/2024-12:00:22] [TRT] [E] 9: LLaMAForCausalLM/transformer/layers/0/attention/PLUGIN_V2_GPTAttention_0: could not find any supported formats consistent with input/output data types
[05/30/2024-12:00:22] [TRT] [E] 9: [pluginV2Builder.cpp::reportPluginError::24] Error Code 9: Internal Error (LLaMAForCausalLM/transformer/layers/0/attention/PLUGIN_V2_GPTAttention_0: could not find any supported formats consistent with input/output data types)
[05/30/2024-12:00:22] [TRT-LLM] [E] Engine building failed, please check the error log.
[05/30/2024-12:00:22] [TRT] [I] Serialized 59 bytes of code generator cache.
[05/30/2024-12:00:22] [TRT] [I] Serialized 0 timing cache entries
[05/30/2024-12:00:22] [TRT-LLM] [I] Timing cache serialized to model.cache
[05/30/2024-12:00:22] [TRT-LLM] [I] Serializing engine to mixtral-8x7b-v0.1-instruct-engine-int8/rank0.engine...
Traceback (most recent call last):
  File "/usr/local/bin/trtllm-build", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 440, in main
    parallel_build(source, build_config, args.output_dir, workers,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 332, in parallel_build
    passed = build_and_save(rank, rank % workers, ckpt_dir,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 298, in build_and_save
    engine.save(output_dir)
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/builder.py", line 566, in save
    serialize_engine(
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/_common.py", line 105, in serialize_engine
    f.write(engine)
TypeError: a bytes-like object is required, not 'NoneType'
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
[05/30/2024-12:00:29] [TRT-LLM] [I] Set bert_attention_plugin to float16.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set gpt_attention_plugin to float16.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set gemm_plugin to None.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set lookup_plugin to None.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set lora_plugin to None.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set moe_plugin to float16.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set context_fmha to True.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set paged_kv_cache to True.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set remove_input_padding to True.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set multi_block_mode to False.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set enable_xqa to True.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set tokens_per_block to 128.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set use_paged_context_fmha to False.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set use_fp8_context_fmha to False.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set multiple_profiles to False.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set paged_state to True.
[05/30/2024-12:00:29] [TRT-LLM] [I] Set streamingllm to False.
[05/30/2024-12:00:29] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. 
It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.
[05/30/2024-12:00:29] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. 

[05/30/2024-12:00:30] [TRT] [I] [MemUsageChange] Init CUDA: CPU +15, GPU +0, now: CPU 651, GPU 423 (MiB)
[05/30/2024-12:00:35] [TRT] [I] [MemUsageChange] Init builder kernel library: CPU +1985, GPU +350, now: CPU 2772, GPU 773 (MiB)
[05/30/2024-12:00:35] [TRT-LLM] [I] Set weight_only_quant_matmul_plugin to bfloat16.
[05/30/2024-12:00:35] [TRT-LLM] [I] Set nccl_plugin to bfloat16.
[05/30/2024-12:00:35] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:35] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT] [W] IElementWiseLayer with inputs LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0 and LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0: first input has type BFloat16 but second input has type Float.
[05/30/2024-12:00:36] [TRT-LLM] [I] Build TensorRT engine Unnamed Network 0
[05/30/2024-12:00:36] [TRT] [W] Unused Input: position_ids
[05/30/2024-12:00:36] [TRT] [W] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32 or Bool.
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor input_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor position_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor last_token_ids, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor kv_cache_block_pointers, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor host_kv_cache_block_pointers, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor sequence_length, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor host_request_types, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor host_past_key_value_lengths, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor context_lengths, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor host_context_lengths, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor host_max_attention_window_sizes, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor host_sink_token_length, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor cache_indirection, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/vocab_embedding/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/vocab_embedding/GATHER_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/0/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/1/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/2/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/3/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/4/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/5/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/6/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/7/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/8/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/9/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/10/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/11/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/12/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/13/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/14/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/15/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/16/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/17/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/18/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/19/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/20/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/21/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/22/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/23/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/24/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/25/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/26/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/27/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/28/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/29/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/30/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/input_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/qkv/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/qkv/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/PLUGIN_V2_GPTAttention_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/dense/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/dense/PLUGIN_V2_WeightOnlyQuantMatmul_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/attention/dense/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/post_layernorm/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/CAST_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/router/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/router/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/CONSTANT_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/CONSTANT_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/PLUGIN_V2_MixtureOfExperts_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/mlp/PLUGIN_V2_AllReduce_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/layers/31/ELEMENTWISE_SUM_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/CONSTANT_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/SHUFFLE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/ELEMENTWISE_POW_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/REDUCE_AVG_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/CONSTANT_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/SHUFFLE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/ELEMENTWISE_SUM_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/UNARY_SQRT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/ELEMENTWISE_DIV_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/SHUFFLE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/transformer/ln_f/ELEMENTWISE_PROD_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/ELEMENTWISE_SUB_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/GATHER_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/SHUFFLE_4_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/SHUFFLE_9_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/CONSTANT_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/MATRIX_MULTIPLY_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/PLUGIN_V2_AllGather_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/SLICE_0_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/SLICE_1_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/SLICE_2_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor LLaMAForCausalLM/lm_head/SLICE_3_output_0, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] Missing scale and zero-point for tensor logits, expect fall back to non-int8 implementation for any layer consuming or producing given tensor
[05/30/2024-12:00:36] [TRT] [W] [RemoveDeadLayers] Input Tensor position_ids is unused or used only at compile-time, but is not being removed.
[05/30/2024-12:00:36] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2814, GPU 799 (MiB)
[05/30/2024-12:00:36] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +10, now: CPU 2815, GPU 809 (MiB)
[05/30/2024-12:00:36] [TRT] [W] TensorRT was linked against cuDNN 8.9.6 but loaded cuDNN 8.9.2
[05/30/2024-12:00:36] [TRT] [I] Global timing cache in use. Profiling results in this builder pass will be stored.
[05/30/2024-12:00:36] [TRT] [E] 9: LLaMAForCausalLM/transformer/layers/0/attention/PLUGIN_V2_GPTAttention_0: could not find any supported formats consistent with input/output data types
[05/30/2024-12:00:36] [TRT] [E] 9: [pluginV2Builder.cpp::reportPluginError::24] Error Code 9: Internal Error (LLaMAForCausalLM/transformer/layers/0/attention/PLUGIN_V2_GPTAttention_0: could not find any supported formats consistent with input/output data types)
[05/30/2024-12:00:36] [TRT-LLM] [E] Engine building failed, please check the error log.
[05/30/2024-12:00:36] [TRT] [I] Serialized 59 bytes of code generator cache.
[05/30/2024-12:00:36] [TRT] [I] Serialized 0 timing cache entries
[05/30/2024-12:00:36] [TRT-LLM] [I] Timing cache serialized to model.cache
[05/30/2024-12:00:36] [TRT-LLM] [I] Serializing engine to mixtral-8x7b-v0.1-instruct-engine-int4/rank0.engine...
Traceback (most recent call last):
  File "/usr/local/bin/trtllm-build", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 440, in main
    parallel_build(source, build_config, args.output_dir, workers,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 332, in parallel_build
    passed = build_and_save(rank, rank % workers, ckpt_dir,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 298, in build_and_save
    engine.save(output_dir)
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/builder.py", line 566, in save
    serialize_engine(
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/_common.py", line 105, in serialize_engine
    f.write(engine)
TypeError: a bytes-like object is required, not 'NoneType'
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
0.9.0
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:02,  3.07it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  3.23it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:01,  3.37it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:01<00:01,  3.87it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:01<00:00,  4.08it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  4.09it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  4.13it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  4.95it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  4.16it/s]
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Traceback (most recent call last):
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 358, in <module>
    main()
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 350, in main
    convert_and_save_hf(args)
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 312, in convert_and_save_hf
    execute(args.workers, [convert_and_save_rank] * world_size, args)
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 318, in execute
    f(args, rank)
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 298, in convert_and_save_rank
    qwen = from_hugging_face(
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/qwen/convert.py", line 884, in from_hugging_face
    config = create_config_from_hugging_face(model_dir,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/qwen/convert.py", line 839, in create_config_from_hugging_face
    rms_norm_eps = hf_config.layer_norm_epsilon
  File "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py", line 263, in __getattribute__
    return super().__getattribute__(key)
AttributeError: 'Qwen2MoeConfig' object has no attribute 'layer_norm_epsilon'
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
0.9.0
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  4.82it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  4.62it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:01,  4.52it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  4.37it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:01<00:00,  4.30it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  4.36it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  4.35it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.18it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  4.69it/s]
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Traceback (most recent call last):
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 358, in <module>
    main()
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 350, in main
    convert_and_save_hf(args)
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 312, in convert_and_save_hf
    execute(args.workers, [convert_and_save_rank] * world_size, args)
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 318, in execute
    f(args, rank)
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 298, in convert_and_save_rank
    qwen = from_hugging_face(
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/qwen/convert.py", line 884, in from_hugging_face
    config = create_config_from_hugging_face(model_dir,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/qwen/convert.py", line 839, in create_config_from_hugging_face
    rms_norm_eps = hf_config.layer_norm_epsilon
  File "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py", line 263, in __getattribute__
    return super().__getattribute__(key)
AttributeError: 'Qwen2MoeConfig' object has no attribute 'layer_norm_epsilon'
src/backend/convert.sh: line 19: y: command not found
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
0.9.0
Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:01,  4.66it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:00<00:01,  4.69it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:00<00:01,  4.72it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:00<00:00,  4.81it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:01<00:00,  4.81it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:01<00:00,  4.64it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:01<00:00,  4.43it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  5.27it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:01<00:00,  4.89it/s]
/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Traceback (most recent call last):
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 358, in <module>
    main()
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 350, in main
    convert_and_save_hf(args)
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 312, in convert_and_save_hf
    execute(args.workers, [convert_and_save_rank] * world_size, args)
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 318, in execute
    f(args, rank)
  File "/root/TensorRT-LLM/examples/qwen/convert_checkpoint.py", line 298, in convert_and_save_rank
    qwen = from_hugging_face(
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/qwen/convert.py", line 884, in from_hugging_face
    config = create_config_from_hugging_face(model_dir,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/qwen/convert.py", line 839, in create_config_from_hugging_face
    rms_norm_eps = hf_config.layer_norm_epsilon
  File "/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py", line 263, in __getattribute__
    return super().__getattribute__(key)
AttributeError: 'Qwen2MoeConfig' object has no attribute 'layer_norm_epsilon'
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
[05/30/2024-12:04:18] [TRT-LLM] [I] Set bert_attention_plugin to float16.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set gpt_attention_plugin to float16.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set gemm_plugin to bfloat16.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set lookup_plugin to None.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set lora_plugin to None.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set moe_plugin to float16.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set context_fmha to True.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set paged_kv_cache to True.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set remove_input_padding to True.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set multi_block_mode to False.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set enable_xqa to True.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set tokens_per_block to 128.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set use_paged_context_fmha to False.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set use_fp8_context_fmha to False.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set multiple_profiles to False.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set paged_state to True.
[05/30/2024-12:04:18] [TRT-LLM] [I] Set streamingllm to False.
[05/30/2024-12:04:18] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. 
It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.
[05/30/2024-12:04:18] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. 

Traceback (most recent call last):
  File "/usr/local/bin/trtllm-build", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 440, in main
    parallel_build(source, build_config, args.output_dir, workers,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 314, in parallel_build
    model_config = PretrainedConfig.from_json_file(
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/modeling_utils.py", line 230, in from_json_file
    with open(config_file) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'qwen-1.5-moe-a2.7b-chat-ckpt-bfloat16/config.json'
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
[05/30/2024-12:04:23] [TRT-LLM] [I] Set bert_attention_plugin to float16.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set gpt_attention_plugin to float16.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set gemm_plugin to None.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set lookup_plugin to None.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set lora_plugin to None.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set moe_plugin to float16.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set context_fmha to True.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set paged_kv_cache to True.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set remove_input_padding to True.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set multi_block_mode to False.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set enable_xqa to True.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set tokens_per_block to 128.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set use_paged_context_fmha to False.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set use_fp8_context_fmha to False.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set multiple_profiles to False.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set paged_state to True.
[05/30/2024-12:04:23] [TRT-LLM] [I] Set streamingllm to False.
[05/30/2024-12:04:23] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. 
It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.
[05/30/2024-12:04:23] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. 

Traceback (most recent call last):
  File "/usr/local/bin/trtllm-build", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 440, in main
    parallel_build(source, build_config, args.output_dir, workers,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 314, in parallel_build
    model_config = PretrainedConfig.from_json_file(
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/modeling_utils.py", line 230, in from_json_file
    with open(config_file) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'qwen-1.5-moe-a2.7b-chat-ckpt-int8/config.json'
[TensorRT-LLM] TensorRT-LLM version: 0.9.0
[05/30/2024-12:04:28] [TRT-LLM] [I] Set bert_attention_plugin to float16.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set gpt_attention_plugin to float16.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set gemm_plugin to None.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set lookup_plugin to None.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set lora_plugin to None.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set moe_plugin to float16.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set mamba_conv1d_plugin to float16.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set context_fmha to True.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set context_fmha_fp32_acc to False.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set paged_kv_cache to True.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set remove_input_padding to True.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set use_custom_all_reduce to False.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set multi_block_mode to False.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set enable_xqa to True.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set attention_qk_half_accumulation to False.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set tokens_per_block to 128.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set use_paged_context_fmha to False.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set use_fp8_context_fmha to False.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set use_context_fmha_for_generation to False.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set multiple_profiles to False.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set paged_state to True.
[05/30/2024-12:04:28] [TRT-LLM] [I] Set streamingllm to False.
[05/30/2024-12:04:28] [TRT-LLM] [W] remove_input_padding is enabled, while max_num_tokens is not set, setting to max_batch_size*max_input_len. 
It may not be optimal to set max_num_tokens=max_batch_size*max_input_len when remove_input_padding is enabled, because the number of packed input tokens are very likely to be smaller, we strongly recommend to set max_num_tokens according to your workloads.
[05/30/2024-12:04:28] [TRT-LLM] [W] remove_input_padding is enabled, while opt_num_tokens is not set, setting to max_batch_size*max_beam_width. 

Traceback (most recent call last):
  File "/usr/local/bin/trtllm-build", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 440, in main
    parallel_build(source, build_config, args.output_dir, workers,
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/commands/build.py", line 314, in parallel_build
    model_config = PretrainedConfig.from_json_file(
  File "/usr/local/lib/python3.10/dist-packages/tensorrt_llm/models/modeling_utils.py", line 230, in from_json_file
    with open(config_file) as f:
FileNotFoundError: [Errno 2] No such file or directory: 'qwen-1.5-moe-a2.7b-chat-ckpt-int4/config.json'
